{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_Python_code_for_mlp_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgGZaPy9vA2e"
      },
      "source": [
        "# Data Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wyUOIo2vvA3c",
        "colab": {}
      },
      "source": [
        "import datetime as dt  # Python standard library datetime  module\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from tqdm import tqdm_notebook\n",
        "import pickle\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "import gc\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "from collections import Counter#<---value count for list\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MS-L5JZVvA3l",
        "colab": {}
      },
      "source": [
        "#Select the target species\n",
        "file_id=\"nutwoo\"\n",
        "bird_name=\"Nuttall's Woodpecker\"\n",
        "bcr_id='32'\n",
        "\n",
        "file_id=\"recwoo\"\n",
        "bird_name=\"Red-cockaded Woodpecker\"\n",
        "bcr_id='27'\n",
        "\n",
        "file_id=\"lewwoo\"\n",
        "bird_name=\"Lewis’s Woodpecker\"\n",
        "bcr_id='9 and 10'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WjAs-1HgvA3t",
        "colab": {}
      },
      "source": [
        "PATH='/content/drive/My Drive/Colab Notebooks/dissertation/'\n",
        "ebird_ss=pd.read_csv(PATH+'ebird_ss_'+file_id+'_add30yMonth.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVhFnX0veORO",
        "colab_type": "text"
      },
      "source": [
        "## Define useful functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GrUf85ygvNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        if col != 'time':\n",
        "            col_type = df[col].dtypes\n",
        "            if col_type in numerics:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "                if str(col_type)[:3] == 'int':\n",
        "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int8)\n",
        "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int16)\n",
        "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)  \n",
        "                else:\n",
        "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float16)\n",
        "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl3nm7MrdKib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "def plot_confusion_matrix(cm,\n",
        "                          classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix very prettily.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(3, 3))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "\n",
        "    # Specify the tick marks and axis text\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    # The data formatting\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    \n",
        "    # Print the text of the matrix, adjusting text colour for display\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_VTWHcadKui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "def Find_Optimal_Cutoff(target, predicted):\n",
        "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
        "    Parameters\n",
        "    ----------\n",
        "    target : Matrix with dependent or target data, where rows are observations\n",
        "\n",
        "    predicted : Matrix with predicted data, where rows are observations\n",
        "\n",
        "    Returns\n",
        "    -------     \n",
        "    list type, with optimal cutoff value\n",
        "    (Source: https://stackoverflow.com/questions/56203889/how-to-get-the-optimal-threshold-from-roc-curve-in-python)   \n",
        "    \"\"\"\n",
        "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "    i = np.arange(len(tpr)) \n",
        "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
        "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
        "\n",
        "    return list(roc_t['threshold']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVXcWTX1Ca_h",
        "colab_type": "text"
      },
      "source": [
        "## Drop/fix NaN values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcGpJalcCH7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check\n",
        "print((ebird_ss.isna().describe().loc['unique']==2).sort_values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBhusA17CNv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if file_id==\"nutwoo\":\n",
        "    # For Nuttall's Woodpecker\n",
        "    print(ebird_ss.prec30_cv.isna().value_counts())\n",
        "    print(ebird_ss.prec180_cv.isna().value_counts())\n",
        "    print(ebird_ss.observation_count.isna().value_counts())\n",
        "    ebird_ss.loc[ebird_ss['prec30_cv'].isna(),'prec30_cv']=0\n",
        "    ebird_ss.loc[ebird_ss['prec180_cv'].isna(),'prec180_cv']=0\n",
        "    ebird_ss.drop(columns=['observation_count'],inplace=True)\n",
        "\n",
        "elif file_id==\"recwoo\":\n",
        "    # For \"Red-cockaded Woodpecker\"\n",
        "    print(ebird_ss.prec30_cv.isna().value_counts())\n",
        "    print(ebird_ss.elevation_median.isna().value_counts())\n",
        "    print(ebird_ss.elevation_sd.isna().value_counts())\n",
        "    print(ebird_ss.observation_count.isna().value_counts())\n",
        "    ebird_ss.drop(columns=['observation_count'],inplace=True)\n",
        "    ebird_ss.dropna(inplace=True)\n",
        "    ebird_ss.reset_index(drop=True,inplace=True)\n",
        "\n",
        "elif file_id==\"lewwoo\":\n",
        "    # For \"Lewis’s Woodpecker\"\n",
        "    print(ebird_ss.prec30_cv.isna().value_counts())\n",
        "    print(ebird_ss.observation_count.isna().value_counts())\n",
        "    ebird_ss.loc[ebird_ss['prec30_cv'].isna(),'prec30_cv']=0\n",
        "    ebird_ss.drop(columns=['observation_count'],inplace=True)\n",
        "\n",
        "else:\n",
        "    print('Missing file_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZGIGeQIdW3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ebird_ss=reduce_mem_usage(ebird_ss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HksvjPJX5LZ-",
        "colab_type": "text"
      },
      "source": [
        "## Set variables for use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYmozHd45Q62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "variables_base=['species_observed',\n",
        "'time_observations_started',\n",
        "'duration_minutes',\n",
        "'effort_distance_km',\n",
        " 'number_observers',\n",
        " 'bio1',\n",
        " 'bio2',\n",
        " 'bio3',\n",
        " 'bio4',\n",
        " 'bio5',\n",
        " 'bio6',\n",
        " 'bio7',\n",
        " 'bio8',\n",
        " 'bio9',\n",
        " 'bio10',\n",
        " 'bio11',\n",
        " 'bio12',\n",
        " 'bio13',\n",
        " 'bio14',\n",
        " 'bio15',\n",
        " 'bio16',\n",
        " 'bio17',\n",
        " 'bio18',\n",
        " 'bio19',\n",
        " 'prec30_mean',\n",
        " 'prec180_mean',\n",
        " 'prec365_mean',\n",
        " 'prec730_mean',\n",
        " 'prec1095_mean',\n",
        " 'prec1460_mean',\n",
        " 'prec1825_mean',\n",
        " 'tmp30_mean',\n",
        " 'tmp30_std',\n",
        " 'tmp180_mean',\n",
        " 'tmp180_std',\n",
        " 'tmp365_mean',\n",
        " 'tmp365_std',\n",
        " 'tmp730_mean',\n",
        " 'tmp730_std',\n",
        " 'tmp1095_mean',\n",
        " 'tmp1095_std',\n",
        " 'tmp1460_mean',\n",
        " 'tmp1460_std',\n",
        " 'tmp1825_mean',\n",
        " 'tmp1825_std',\n",
        " 'tmax30_mean',\n",
        " 'tmax30_std',\n",
        " 'tmax180_mean',\n",
        " 'tmax180_std',\n",
        " 'tmax365_mean',\n",
        " 'tmax365_std',\n",
        " 'tmax730_mean',\n",
        " 'tmax730_std',\n",
        " 'tmax1095_mean',\n",
        " 'tmax1095_std',\n",
        " 'tmax1460_mean',\n",
        " 'tmax1460_std',\n",
        " 'tmax1825_mean',\n",
        " 'tmax1825_std',\n",
        " 'tmin30_mean',\n",
        " 'tmin30_std',\n",
        " 'tmin180_mean',\n",
        " 'tmin180_std',\n",
        " 'tmin365_mean',\n",
        " 'tmin365_std',\n",
        " 'tmin730_mean',\n",
        " 'tmin730_std',\n",
        " 'tmin1095_mean',\n",
        " 'tmin1095_std',\n",
        " 'tmin1460_mean',\n",
        " 'tmin1460_std',\n",
        " 'tmin1825_mean',\n",
        " 'tmin1825_std',\n",
        " 'prec30_cv',\n",
        " 'prec180_cv',\n",
        " 'prec365_cv',\n",
        " 'prec730_cv',\n",
        " 'prec1095_cv',\n",
        " 'prec1460_cv',\n",
        " 'prec1825_cv',\n",
        " 'tmin_30y_monthly',\n",
        " 'tmax_30y_monthly',\n",
        " 'tavg_30y_monthly',\n",
        " 'prec_30y_monthly',\n",
        " 'srad_30y_monthly',\n",
        " 'wind_30y_monthly',\n",
        " 'vapr_30y_monthly',\n",
        " 'pland_00_water',\n",
        " 'pland_01_evergreen_needleleaf',\n",
        " 'pland_02_evergreen_broadleaf',\n",
        " 'pland_03_deciduous_needleleaf',#<--remove for nutwoo\n",
        " 'pland_04_deciduous_broadleaf',\n",
        " 'pland_05_mixed_forest',\n",
        " 'pland_06_closed_shrubland',\n",
        " 'pland_07_open_shrubland',\n",
        " 'pland_08_woody_savanna',\n",
        " 'pland_09_savanna',\n",
        " 'pland_10_grassland',\n",
        " 'pland_11_wetland',\n",
        " 'pland_12_cropland',\n",
        " 'pland_13_urban',\n",
        " 'pland_14_mosiac',\n",
        " 'pland_15_barren',\n",
        "'elevation_median',\n",
        "'elevation_sd'\n",
        "]\n",
        "if file_id==\"nutwoo\":\n",
        "    variables_base.remove('pland_03_deciduous_needleleaf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ls-oEZ55G6m",
        "colab_type": "text"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEP7gQnwoovz",
        "colab_type": "text"
      },
      "source": [
        "## Data normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9_Sq9ifosGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "variables=variables_base.copy()\n",
        "x_base = ebird_ss[variables].values #returns a numpy array\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(x_base)\n",
        "ebird_ss_nn = min_max_scaler.transform(x_base)\n",
        "ebird_ss_nn = pd.DataFrame(ebird_ss_nn,columns=variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhmE2Ld0owZu",
        "colab_type": "text"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F2gUYTIow3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class CustomLinear(nn.Module):\n",
        "    def __init__(self, in_channels, bias=True, p=0.1):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_channels, 512, bias)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=512)\n",
        "        self.fc1 = nn.Linear(512,1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxsXIbNUow_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T97nvIRlvXD_",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwx5HjufvJBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=72).split(ebird_ss_nn, ebird_ss_nn[['species_observed']]))\n",
        "\n",
        "batch_size = 512\n",
        "train_epochs = 100\n",
        "\n",
        "AUC_ls = []\n",
        "result_ls = []\n",
        "model_tuning_ls_test = []\n",
        "test_preds_ls = []\n",
        "\n",
        "for i, (train_idx, test_idx) in enumerate(splits):\n",
        "    model_tuning_ls_valid = []\n",
        "    print(f'+++++++Start for {i+1} fold test data+++++++')\n",
        "    variables=variables_base.copy()\n",
        "\n",
        "    train_x = ebird_ss_nn.iloc[train_idx,:]\n",
        "    test_x = ebird_ss_nn.iloc[test_idx,:]\n",
        "\n",
        "    train_x=train_x[variables]\n",
        "    test_x=test_x[variables]\n",
        "    train_X=train_x[variables[1:]]\n",
        "    train_y=train_x['species_observed']\n",
        "    test_X=test_x[variables[1:]]\n",
        "    test_y=test_x['species_observed']\n",
        "\n",
        "\n",
        "    train_preds = np.zeros((len(train_X)))\n",
        "    test_preds=[]\n",
        "\n",
        "    x_test_cuda = torch.tensor(test_X.values, dtype=torch.float32).cuda()\n",
        "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    splits_val = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=72).split(train_X, train_y))\n",
        "    for j, (train2_idx, val_idx) in enumerate(splits_val):\n",
        "        print(f'=======Start for {j+1} fold valid data in {i+1} fold test data=====')\n",
        "\n",
        "        train2_x = train_x.iloc[train2_idx,:]\n",
        "        valid_x = train_x.iloc[val_idx,:]\n",
        "\n",
        "        train2_X=train2_x[variables[1:]]\n",
        "        train2_y=train2_x['species_observed']\n",
        "        valid_X=valid_x[variables[1:]]\n",
        "        valid_y=valid_x['species_observed']\n",
        "\n",
        "        x_train_fold = torch.tensor(train2_X.to_numpy(), dtype=torch.float32).cuda()\n",
        "        y_train_fold = torch.tensor(train2_y[:, np.newaxis], dtype=torch.float32).cuda()\n",
        "\n",
        "        x_val_fold = torch.tensor(valid_X.to_numpy(), dtype=torch.float32).cuda()\n",
        "        y_val_fold = torch.tensor(valid_y[:, np.newaxis], dtype=torch.float32).cuda()\n",
        "        \n",
        "        if file_id==\"nutwoo\":\n",
        "            net=CustomLinear(103)\n",
        "        else:\n",
        "            net=CustomLinear(104)\n",
        "        model = net\n",
        "        model.cuda()\n",
        "        \n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
        "        optimizer = torch.optim.Adam(model.parameters(),lr=0.001,)\n",
        "        \n",
        "        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
        "        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
        "        \n",
        "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "        \n",
        "        print(f'Fold {i + 1}-Validation {j + 1}')\n",
        "        \n",
        "        ave_val_min=999.\n",
        "        count=0\n",
        "        for epoch in range(train_epochs):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            model.train()\n",
        "            avg_loss = 0.\n",
        "\n",
        "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
        "                 y_pred = model(x_batch)\n",
        "                loss = loss_fn(y_pred, y_batch)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                avg_loss += loss.item() / len(train_loader)\n",
        "            \n",
        "            model.eval()\n",
        "            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
        "            test_preds_fold = np.zeros(len(test_x))\n",
        "            avg_val_loss = 0.\n",
        "\n",
        "            for k, (x_batch, y_batch) in enumerate(valid_loader):\n",
        "                y_pred = model(x_batch).detach()\n",
        "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
        "                valid_preds_fold[k * batch_size:(k+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
        "            \n",
        "            elapsed_time = time.time() - start_time \n",
        "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
        "                epoch + 1, train_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
        "            \n",
        "            if ave_val_min >= avg_val_loss:\n",
        "                ave_val_min = avg_val_loss\n",
        "                best_model = copy.deepcopy(model)\n",
        "                best_epoch = epoch + 1\n",
        "                count=0\n",
        "            else:\n",
        "                count+=1\n",
        "            if count==10:\n",
        "                print('Not updated during the last 10 epochs')\n",
        "                break\n",
        "\n",
        "        for l, (x_batch,) in enumerate(test_loader):\n",
        "            y_pred = best_model(x_batch).detach()\n",
        "            test_preds_fold[l * batch_size:(l+1) * batch_size] += sigmoid(y_pred.cpu().numpy())[:, 0]\n",
        "        print(f'!!!Best model is at Epoch {best_epoch}!!!')\n",
        "\n",
        "        test_preds.append(test_preds_fold)\n",
        "    test_preds_ls.append(test_preds)\n",
        "\n",
        "with open('test_preds_ls_'+file_id+'.pkl', 'wb') as handle:\n",
        "    pickle.dump(test_preds_ls, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ_8YzqgwLCS",
        "colab_type": "text"
      },
      "source": [
        "## Show ROC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQI40bDgvJTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=72).split(ebird_ss, ebird_ss[['species_observed']]))\n",
        "\n",
        "fig,ax=plt.subplots(1,figsize=(5,4))\n",
        "\n",
        "AUC_ls=[]\n",
        "for i, ((train_idx, test_idx),test_preds) in enumerate(zip(splits,test_preds_ls)):\n",
        "    variables=variables_base.copy()\n",
        "    \n",
        "    train_x = ebird_ss.iloc[train_idx,:]\n",
        "    test_x = ebird_ss.iloc[test_idx,:]\n",
        "\n",
        "    train_x=train_x[variables]\n",
        "    test_x=test_x[variables]\n",
        "\n",
        "    train_X=train_x[variables[1:]]\n",
        "    train_y=train_x['species_observed']\n",
        "    test_X=test_x[variables[1:]]\n",
        "    test_y=test_x['species_observed']\n",
        "\n",
        "    pred = np.mean(test_preds,axis=0)\n",
        "    tmp1=pd.DataFrame(pred,columns=[\"prediction\"])\n",
        "    tmp2=pd.DataFrame(test_y.values.reshape(-1,1),columns=['Actual']).astype(int)\n",
        "    df_RF=pd.concat([tmp1,tmp2],axis=1)\n",
        "    \n",
        "    rf_obs=test_x.sort_values('species_observed').astype(float)['species_observed'].values\n",
        "    rf_pred=df_RF['prediction'].values\n",
        "\n",
        "\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(rf_obs,rf_pred, pos_label=None)\n",
        "    AUC=metrics.auc(fpr, tpr)\n",
        "    AUC_ls.append(AUC)\n",
        "    print(f'AUC of valid data:{AUC:0.3f}')\n",
        "\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    auc_val=str(round(metrics.auc(fpr, tpr),3)).ljust(5, '0')\n",
        "    plt.plot(fpr,tpr,label=f'[AUC={auc_val}] {i+1}-fold')\n",
        "    plt.xlabel('False positive rate (1-specificity)')\n",
        "    plt.ylabel('True positive rate (sensitivity)')\n",
        "    plt.title('ROC curve')\n",
        "\n",
        "print(f'Mean: {np.mean(AUC_ls):0.4f}')\n",
        "print(f'SD: {np.std(AUC_ls):0.4f}')\n",
        "plt.legend(loc='best')\n",
        "fig.savefig(f'mlp_AUC_{file_id}.png',bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeoXrNvLwUi1",
        "colab_type": "text"
      },
      "source": [
        "## Sensitivity, specificity, threshold, confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEYeFdEmvJbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=72).split(ebird_ss, ebird_ss[['species_observed']]))\n",
        "\n",
        "sensitivity_ls=[]\n",
        "specificity_ls=[]\n",
        "threshold_ls=[]\n",
        "\n",
        "for i, ((train_idx, test_idx),test_preds) in enumerate(zip(splits,test_preds_ls)):   \n",
        "    variables=variables_base.copy()\n",
        "    \n",
        "    train_x = ebird_ss.iloc[train_idx,:]\n",
        "    test_x = ebird_ss.iloc[test_idx,:]\n",
        "\n",
        "    train_x=train_x[variables]\n",
        "    test_x=test_x[variables]\n",
        "\n",
        "    train_X=train_x[variables[1:]]\n",
        "    train_y=train_x['species_observed']\n",
        "    test_X=test_x[variables[1:]]\n",
        "    test_y=test_x['species_observed']\n",
        "\n",
        "    pred = np.mean(test_preds,axis=0)\n",
        "    tmp1=pd.DataFrame(pred,columns=[\"prediction\"])\n",
        "    tmp2=pd.DataFrame(test_y.values.reshape(-1,1),columns=['Actual']).astype(int)\n",
        "    df_RF=pd.concat([tmp1,tmp2],axis=1)\n",
        "    \n",
        "    rf_obs=test_x.sort_values('species_observed').astype(float)['species_observed'].values\n",
        "    rf_pred=df_RF['prediction'].values\n",
        "    \n",
        "    threshold = Find_Optimal_Cutoff(rf_obs,rf_pred)\n",
        "    print(f'threshold: {np.round(threshold[0],3)}')\n",
        "    threshold_ls.append(threshold)\n",
        "    rf_pred_disc=rf_pred>threshold[0]\n",
        "\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    cm = confusion_matrix(rf_obs,rf_pred_disc,labels=[1,0])\n",
        "\n",
        "    sensitivity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "    print('Sensitivity : ', np.round(sensitivity1,3) )\n",
        "    sensitivity_ls.append(sensitivity1)\n",
        "\n",
        "    specificity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
        "    print('Specificity : ', np.round(specificity1,3))\n",
        "    specificity_ls.append(specificity1)\n",
        "    print(f'{np.round(sensitivity1,3)}&{np.round(specificity1,3)}&{np.round(threshold[0],3)}')\n",
        "\n",
        "    classes=['True','False']\n",
        "    plot_confusion_matrix(cm,classes,normalize=False)\n",
        "    \n",
        "    if i==0:\n",
        "        plt.savefig(f'mlp_cm_{file_id}_{i+1}_fold.png',bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(f'Threshold: {np.mean(threshold_ls):0.4f}')\n",
        "print(f'Sensitivity: {np.mean(sensitivity_ls):0.4f}')\n",
        "print(f'Specificity: {np.mean(specificity_ls):0.4f}')\n",
        "\n",
        "\n",
        "print(f'Threshold: {np.std(threshold_ls):0.4f}')\n",
        "print(f'Sensitivity: {np.std(sensitivity_ls):0.4f}')\n",
        "print(f'Specificity: {np.std(specificity_ls):0.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-DEuzk-wgyu",
        "colab_type": "text"
      },
      "source": [
        "## Calibration plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4E8Lma-vJi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=72).split(ebird_ss, ebird_ss[['species_observed']]))\n",
        "\n",
        "fig,axes=plt.subplots(1,3,figsize=(15,5))\n",
        "for i, ((train_idx, test_idx),test_preds) in enumerate(zip(splits,test_preds_ls)):\n",
        "    variables=variables_base.copy()\n",
        "    \n",
        "    train_x = ebird_ss.iloc[train_idx,:]\n",
        "    test_x = ebird_ss.iloc[test_idx,:]\n",
        "\n",
        "    train_x=train_x[variables]\n",
        "    test_x=test_x[variables]\n",
        "\n",
        "    train_X=train_x[variables[1:]]\n",
        "    train_y=train_x['species_observed']\n",
        "    test_X=test_x[variables[1:]]\n",
        "    test_y=test_x['species_observed']\n",
        "\n",
        "    pred = np.mean(test_preds,axis=0)\n",
        "    tmp1=pd.DataFrame(pred,columns=[\"prediction\"])\n",
        "    tmp2=pd.DataFrame(test_y.values.reshape(-1,1),columns=['Actual']).astype(int)\n",
        "    df_RF=pd.concat([tmp1,tmp2],axis=1)\n",
        "    \n",
        "    rf_obs=test_x.sort_values('species_observed').astype(float)['species_observed'].values\n",
        "    rf_pred=df_RF['prediction'].values\n",
        "\n",
        "    bin=0.05\n",
        "    df_calib=pd.DataFrame([pd.cut(rf_pred, bins=np.arange(0,1+bin,bin),include_lowest=False),rf_obs],\n",
        "                        columns=('prediction','observed'))\n",
        "\n",
        "    scatter=axes[0].scatter(np.arange(bin/2,1.00+bin/2,bin),\n",
        "            df_calib.groupby('prediction').mean()['observed'].values,\n",
        "                alpha=0.5,label=f'{i+1}-fold')\n",
        "    axes[0].set_xlabel('Predicted encounter rate')\n",
        "    axes[0].set_ylabel('Observed encounter rate')\n",
        "    axes[0].set_title(f'Calibration plot (group size = {bin})')\n",
        "    axes[0].plot([0,1],[0,1],'k--')\n",
        "\n",
        "    bin=0.02\n",
        "    df_calib=pd.DataFrame([pd.cut(rf_pred, bins=np.arange(0,1+bin,bin),include_lowest=False),rf_obs],\n",
        "                        columns=('prediction','observed'))\n",
        "\n",
        "    scatter=axes[1].scatter(np.arange(bin/2,1.00+bin/2,bin),\n",
        "            df_calib.groupby('prediction').mean()['observed'].values,\n",
        "                alpha=0.5,label=f'{i+1}-fold')\n",
        "    axes[1].set_xlabel('Predicted encounter rate')\n",
        "    axes[1].set_ylabel('Observed encounter rate')\n",
        "    axes[1].set_title(f'Calibration plot (group size = {bin})')\n",
        "    axes[1].plot([0,1],[0,1],'k--')\n",
        "\n",
        "    axes[2].plot(np.arange(bin/2,1.00+bin/2,bin),\n",
        "                df_calib['prediction'].value_counts(normalize=True).sort_index().values,alpha=0.5,marker=\"*\",label=f'{i+1}-fold')\n",
        "    axes[2].set_xlabel('Predicted encounter rate')\n",
        "    axes[2].set_ylabel('Frequency (percentage)')\n",
        "    axes[2].set_title(f'Distribution of the prediction (group size = {bin})')\n",
        "\n",
        "axes[0].legend(loc='best')\n",
        "axes[1].legend(loc='best')\n",
        "axes[2].legend(loc='best')\n",
        "fig.savefig(f'mlp_calib_all_{file_id}.png',bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}