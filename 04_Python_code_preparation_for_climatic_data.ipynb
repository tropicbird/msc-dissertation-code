{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Python_code_preparation_for_climatic_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmP9_jjdO2Rp",
        "colab_type": "text"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60ZW1nvqpIgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install netcdf4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Joyux3qo9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install swifter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK4kTHQMvRPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install rasterio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk1v_tgevY4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install earthpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKzP6GDUnoiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime as dt  \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from tqdm import tqdm_notebook\n",
        "import swifter\n",
        "import pickle\n",
        "import os\n",
        "import rasterio as rio\n",
        "import earthpy as et"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7lOoTeBmDMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH= os.path.join(\"/content\", \"drive\", \"My Drive\", \"Colab Notebooks\", \"dissertation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Lk8FRFjbTB",
        "colab_type": "text"
      },
      "source": [
        "## Select species"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9slrJJ15Ftzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_id=\"nutwoo\"\n",
        "bc_id=32\n",
        "\n",
        "# file_id=\"recwoo\"\n",
        "# bc_id=27\n",
        "\n",
        "# file_id=\"lewwoo\"\n",
        "# bc_id=\"9n10\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C8fYyBtaOkX",
        "colab_type": "text"
      },
      "source": [
        "#Download climatic data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGO1o1VwPBL8",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the long term climatic data (Bioclimatic data (30 years))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9dYiKRNDkDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download 30 years data\n",
        "print('Start')\n",
        "urllib.request.urlretrieve('https://biogeo.ucdavis.edu/data/worldclim/v2.1/base/wc2.1_10m_bio.zip','wc2.1_10m_bio.zip')\n",
        "print('End')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6GWdv7uE2VI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('wc2.1_10m_bio.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wofsj7nYKcDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = rio.open('/content/wc2.1_10m_bio_1.tif')\n",
        "src.closed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ-kkcHUMFsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(src.read(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OMGnNzMGmp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src.meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN8kzzDGwstw",
        "colab_type": "text"
      },
      "source": [
        "## Prepare average 30 climate data of each month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFwrxvA2xDKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download\n",
        "print('Download Start')\n",
        "urllib.request.urlretrieve('http://biogeo.ucdavis.edu/data/worldclim/v2.1/base/wc2.1_10m_tmin.zip','wc2.1_10m_tmin.zip')\n",
        "urllib.request.urlretrieve('http://biogeo.ucdavis.edu/data/worldclim/v2.1/base/wc2.1_10m_tmax.zip','wc2.1_10m_tmax.zip')\n",
        "urllib.request.urlretrieve('http://biogeo.ucdavis.edu/data/worldclim/v2.1/base/wc2.1_10m_tavg.zip','wc2.1_10m_tavg.zip')\n",
        "urllib.request.urlretrieve('http://biogeo.ucdavis.edu/data/worldclim/v2.1/base/wc2.1_10m_prec.zip','wc2.1_10m_prec.zip')\n",
        "urllib.request.urlretrieve('http://biogeo.ucdavis.edu/data/worldclim/v2.1/base/wc2.1_10m_srad.zip','wc2.1_10m_srad.zip')\n",
        "urllib.request.urlretrieve('http://biogeo.ucdavis.edu/data/worldclim/v2.1/base/wc2.1_10m_wind.zip','wc2.1_10m_wind.zip')\n",
        "urllib.request.urlretrieve('http://biogeo.ucdavis.edu/data/worldclim/v2.1/base/wc2.1_10m_vapr.zip','wc2.1_10m_vapr.zip')\n",
        "print('Download End')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y3hWeGUx18z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('wc2.1_10m_tmin.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile('wc2.1_10m_tmax.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile('wc2.1_10m_tavg.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile('wc2.1_10m_prec.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile('wc2.1_10m_srad.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile('wc2.1_10m_wind.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "with zipfile.ZipFile('wc2.1_10m_vapr.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvFT5qMGPJ1U",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the short term climatic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VNO1guURMr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Start')\n",
        "for year in range(2005,2020):\n",
        "    year=str(year)\n",
        "    print(f'Downloading {year}')\n",
        "    url1='ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_precip/precip.'+year+'.nc'\n",
        "    url2='ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_temp/tmax.'+year+'.nc'\n",
        "    url3='ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_temp/tmin.'+year+'.nc'\n",
        "\n",
        "    #---Save to Google Drive---\n",
        "    # urllib.request.urlretrieve(url1, PATH+'/precip.'+year+'.nc')\n",
        "    # urllib.request.urlretrieve(url2, PATH+'/tmax.'+year+'.nc')\n",
        "    # urllib.request.urlretrieve(url3, PATH+'/tmin.'+year+'.nc')\n",
        "    #--------------------------\n",
        "    \n",
        "    urllib.request.urlretrieve(url1, 'precip.'+year+'.nc')\n",
        "    urllib.request.urlretrieve(url2, 'tmax.'+year+'.nc')\n",
        "    urllib.request.urlretrieve(url3, 'tmin.'+year+'.nc')\n",
        "print('End')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj7xKTEdj_Ds",
        "colab_type": "text"
      },
      "source": [
        "# Load eBird data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8gZl29dQjJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp=pd.read_csv(os.path.join(PATH,\"ebd_\"+file_id+\"_bcr\"+str(bc_id)+\"_zf.csv\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVZqd5bYl4hE",
        "colab_type": "text"
      },
      "source": [
        "# Data modification for the short term climatic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgI0Uk1Avt_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "urllib.request.urlretrieve('ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_temp/tmin.2019.nc', 'tmin.2019.nc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNVfLDlnlAlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file=Dataset('tmin.2019.nc','r')\n",
        "ls_precip_files=[ Dataset('precip.'+str(y)+'.nc','r') for y in range(2005,2020)]\n",
        "ls_tmax_files=[ Dataset('tmax.'+str(y)+'.nc','r') for y in range(2005,2020)]\n",
        "ls_tmin_files=[ Dataset('tmin.'+str(y)+'.nc','r') for y in range(2005,2020)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTZxvQPQajLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls_lon=file.variables['lon'][:]\n",
        "ls_lat=file.variables['lat'][:]\n",
        "\n",
        "print('Step1')\n",
        "res1 = dict(zip(ls_lat, np.arange(len(ls_lat)))) \n",
        "res2 = dict(zip(ls_lon, np.arange(len(ls_lon)))) \n",
        "\n",
        "print('Step2')\n",
        "tmp['year_month']=tmp['observation_date'].apply(lambda x: x.replace('-','')[:-2])\n",
        "tmp['month']=tmp['observation_date'].apply(lambda x: int(x.replace('-','')[4:-2]))##<----Added!!!\n",
        "tmp['day']=tmp['observation_date'].apply(lambda x: int(x.replace('-','')[-2:]))\n",
        "tmp['longitude360']=tmp['longitude']%360\n",
        "\n",
        "\n",
        "print('Step2.1')\n",
        "lat_max=tmp['latitude'].max()\n",
        "lat_max_close=min(res1, key=lambda y:abs(y-lat_max))\n",
        "lat_max_close_num=res1[min(res1, key=lambda y:abs(y-lat_max))]\n",
        "\n",
        "lat_min=tmp['latitude'].min()\n",
        "lat_min_close=min(res1, key=lambda y:abs(y-lat_min))\n",
        "lat_min_close_num=res1[min(res1, key=lambda y:abs(y-lat_min))]\n",
        "\n",
        "tmp['lat_cate']=pd.cut(tmp.latitude, bins=np.arange(lat_min_close-0.25, lat_max_close+0.25+0.5,0.5))\n",
        "tmp['lat_num']=pd.cut(tmp.latitude, bins=np.arange(lat_min_close-0.25, lat_max_close+0.25+0.5,0.5),labels=np.arange(lat_min_close_num, lat_max_close_num-1,-1))\n",
        "\n",
        "print('Step2.2')\n",
        "lon_max=tmp['longitude360'].max()\n",
        "lon_max_close=min(res2, key=lambda y:abs(y-lon_max))\n",
        "lon_max_close_num=res2[min(res2, key=lambda y:abs(y-lon_max))]\n",
        "\n",
        "lon_min=tmp['longitude360'].min()\n",
        "lon_min_close=min(res2, key=lambda y:abs(y-lon_min))\n",
        "lon_min_close_num=res2[min(res2, key=lambda y:abs(y-lon_min))]\n",
        "\n",
        "tmp['lon_cate']=pd.cut(tmp.longitude360, bins=np.arange(lon_min_close-0.25, lon_max_close+0.25+0.5,0.5))\n",
        "tmp['lon_num']=pd.cut(tmp.longitude360, bins=np.arange(lon_min_close-0.25, lon_max_close+0.25+0.5,0.5),labels=np.arange(lon_min_close_num, lon_max_close_num+1,1))\n",
        "\n",
        "print('Step3')\n",
        "tmp['day_since_2009']=int(99999)\n",
        "tmp.loc[tmp['year']<=2012,'day_since_2009']=(tmp['day_of_year']+(tmp['year']-2009)*365)[tmp['year']<=2012]\n",
        "tmp.loc[(tmp['year']<=2016)&(tmp['year']>2012),'day_since_2009']=(tmp['day_of_year']+(tmp['year']-2009)*365+1)[(tmp['year']<=2016)&(tmp['year']>2012)]\n",
        "tmp.loc[(tmp['year']>2016),'day_since_2009']=(tmp['day_of_year']+(tmp['year']-2009)*365+2)[tmp['year']>2016]\n",
        "\n",
        "tmp['day_since_2007']=int(99999)\n",
        "tmp.loc[tmp['year']<=2012,'day_since_2007']=(tmp['day_of_year']+(tmp['year']-2009)*365)[tmp['year']<=2012]+731\n",
        "tmp.loc[(tmp['year']<=2016)&(tmp['year']>2012),'day_since_2007']=(tmp['day_of_year']+(tmp['year']-2009)*365+1)[(tmp['year']<=2016)&(tmp['year']>2012)]+731\n",
        "tmp.loc[(tmp['year']>2016),'day_since_2007']=(tmp['day_of_year']+(tmp['year']-2009)*365+2)[tmp['year']>2016]+731\n",
        "\n",
        "tmp['day_since_2005']=int(99999)\n",
        "tmp.loc[tmp['year']<=2012,'day_since_2005']=(tmp['day_of_year']+(tmp['year']-2009)*365)[tmp['year']<=2012]+731+730\n",
        "tmp.loc[(tmp['year']<=2016)&(tmp['year']>2012),'day_since_2005']=(tmp['day_of_year']+(tmp['year']-2009)*365+1)[(tmp['year']<=2016)&(tmp['year']>2012)]+731+730\n",
        "tmp.loc[(tmp['year']>2016),'day_since_2005']=(tmp['day_of_year']+(tmp['year']-2009)*365+2)[tmp['year']>2016]+731+730\n",
        "\n",
        "print('Step4')\n",
        "res1_reverse={v:float(k) for k, v in res1.items()}\n",
        "res2_reverse={v:float(k) for k, v in res2.items()}\n",
        "\n",
        "tmp['lat_cate_val']=tmp['lat_num'].apply(lambda x:res1_reverse[x])\n",
        "tmp=tmp.astype({'lat_cate_val': 'float64'})\n",
        "\n",
        "tmp['lon_cate_val']=tmp['lon_num'].apply(lambda x:res2_reverse[x])\n",
        "tmp=tmp.astype({'lon_cate_val': 'float64'})\n",
        "tmp['lon_cate_val']=tmp['lon_cate_val']%-360\n",
        "\n",
        "print('Step5')\n",
        "\n",
        "use_col=[\"checklist_id\",'lat_num', 'lon_num','lat_cate_val','lon_cate_val','year','month','day_of_year','day_since_2005']\n",
        "checklist=tmp[use_col]\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKrzXDtF_oOe",
        "colab_type": "text"
      },
      "source": [
        "# Save lat_num and lon_num dictionary data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlLr_ESHyuug",
        "colab_type": "text"
      },
      "source": [
        "## Bioclimatic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0S3vMMj1MOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls_bio=[]\n",
        "for i in range(1,20):\n",
        "    ls_bio.append('wc2.1_10m_bio_'+ str(i) +'.tif')\n",
        "ls_bio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeyQu04J1Kg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=checklist.groupby(['lat_cate_val','lon_cate_val']).size().reset_index(name='count').query('count>0').reset_index(drop=True)\n",
        "display(test)\n",
        "coordinates=[(lon,lat) for lon, lat in zip(test['lon_cate_val'],test['lat_cate_val'])]\n",
        "dic={}\n",
        "for en, bio in tqdm_notebook(enumerate(ls_bio)):\n",
        "    ls_val=[]\n",
        "    src = rio.open('/content/'+bio)\n",
        "    for i in src.sample(coordinates):\n",
        "        if i[0]<-1000000000:\n",
        "            ls_val.append(np.nan)\n",
        "        else:\n",
        "            ls_val.append(i[0])\n",
        "    bio_name='bio'+str(en+1)\n",
        "    dic[bio_name]=ls_val\n",
        "\n",
        "df_bioclimatic30y=pd.concat([test,pd.DataFrame(dic)],axis=1)\n",
        "display(df_bioclimatic30y)\n",
        "\n",
        "with open(os.path.join(PATH,'df_bioclimatic30y'+file_id+'.pkl'), 'wb') as handle:\n",
        "    pickle.dump(df_bioclimatic30y, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dseqAn8dy2GU",
        "colab_type": "text"
      },
      "source": [
        "## Monthly 30y climate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o18tkYZxNFSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=checklist.groupby(['lat_cate_val','lon_cate_val']).size().reset_index(name='count').query('count>0').reset_index(drop=True)\n",
        "display(test)\n",
        "test['tttt']=1\n",
        "df_month=pd.DataFrame({'month':[1,2,3,4,5,6,7,8,9,10,11,12]})\n",
        "df_month['tttt']=1\n",
        "test=pd.merge(df_month,test,on=['tttt']).drop('tttt', axis=1)\n",
        "display(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEq-_2x404bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clm30y_monthly(var, checklist):\n",
        "    print(f'Start:{var}')\n",
        "    ls=[]\n",
        "    for i in range(1,13):\n",
        "        ls.append('wc2.1_10m_' + var + '_'+ str(i).zfill(2) +'.tif')\n",
        "\n",
        "    # test=checklist.groupby(['lat_cate_val','lon_cate_val','month']).size().reset_index(name='count').query('count>0').reset_index(drop=True)\n",
        "    # test=test.sort_values(by=['month'],ignore_index=True)\n",
        "    # display(test)\n",
        "\n",
        "    test=checklist.groupby(['lat_cate_val','lon_cate_val']).size().reset_index(name='count').query('count>0').reset_index(drop=True)\n",
        "    display(test)\n",
        "    test['tttt']=1\n",
        "    df_month=pd.DataFrame({'month':[1,2,3,4,5,6,7,8,9,10,11,12]})\n",
        "    df_month['tttt']=1\n",
        "    test=pd.merge(df_month,test,on=['tttt']).drop('tttt', axis=1)\n",
        "    display(test)\n",
        "\n",
        "    #coordinates=[[(lon,lat),m] for lon, lat,m in zip(test['lon_cate_val'],test['lat_cate_val'],test['month'])]\n",
        "    dic={}\n",
        "    ls_val=[]\n",
        "    for m, clm_data in tqdm_notebook(enumerate(ls)):\n",
        "        #ls_val=[]\n",
        "        src = rio.open('/content/'+clm_data)\n",
        "        coordinates=[(lon,lat) for lon, lat in zip(test['lon_cate_val'][test['month']==m+1],test['lat_cate_val'][test['month']==m+1])]\n",
        "        for i in src.sample(coordinates):\n",
        "            if i[0]<-1000000000: #欠損値の場合[-3.400000e+38]のため\n",
        "                ls_val.append(np.nan)\n",
        "            else:\n",
        "                ls_val.append(i[0])\n",
        "        #m_name=str(en+1)\n",
        "        #dic[m_name]=ls_val\n",
        "\n",
        "\n",
        "    dic[var+'_30y_monthly']=ls_val\n",
        "    data_30y_monthly=pd.concat([test,pd.DataFrame(dic)],axis=1)\n",
        "    display(data_30y_monthly)\n",
        "\n",
        "    with open(os.path.join(PATH,var+'_30y_monthly'+file_id+'.pkl'), 'wb') as handle:\n",
        "        pickle.dump(data_30y_monthly, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    print(f'End:{var}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrrz_SjyKTsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clm30y_monthly('tmin', checklist)\n",
        "clm30y_monthly('tmax', checklist)\n",
        "clm30y_monthly('tavg', checklist)\n",
        "clm30y_monthly('prec', checklist)\n",
        "clm30y_monthly('srad', checklist)\n",
        "clm30y_monthly('wind', checklist)\n",
        "clm30y_monthly('vapr', checklist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNJ844310Pc2",
        "colab_type": "text"
      },
      "source": [
        "## Precip for the short term climatic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPmMPNn2od4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=checklist.groupby(['lat_num','lon_num']).size().reset_index(name='count').query('count>0').reset_index(drop=True)\n",
        "display(test)\n",
        "dic={}\n",
        "for lat, lon in tqdm_notebook(zip(test['lat_num'],test['lon_num'])):\n",
        "    ls=[]\n",
        "    for file in ls_precip_files:\n",
        "        ls.extend(file.variables['precip'][:,lat,lon])\n",
        "        #print('done')\n",
        "    dic[(lat,lon)]=ls\n",
        "\n",
        "with open(os.path.join(PATH,'dic_prec_2005_2019'+file_id+'.pkl'), 'wb') as handle:\n",
        "    pickle.dump(dic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2nBwUGW0nQW",
        "colab_type": "text"
      },
      "source": [
        "## tmax for the short term climatic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpnyoH8LOR7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=checklist.groupby(['lat_num','lon_num']).size().reset_index(name='count').query('count>0').reset_index(drop=True)\n",
        "display(test)\n",
        "dic_tmax={}\n",
        "for lat, lon in tqdm_notebook(zip(test['lat_num'],test['lon_num'])):\n",
        "    ls=[]\n",
        "    for file in ls_tmax_files:\n",
        "        ls.extend(file.variables['tmax'][:,lat,lon])\n",
        "        #print('done')\n",
        "    dic_tmax[(lat,lon)]=ls\n",
        "\n",
        "import pickle\n",
        "with open(os.path.join(PATH,'dic_tmax_2005_2019'+file_id+'.pkl'), 'wb') as handle:\n",
        "    pickle.dump(dic_tmax, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C45YcwMH0uyg",
        "colab_type": "text"
      },
      "source": [
        "## tmin for the short term climatic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzzL4JiXf_rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=checklist.groupby(['lat_num','lon_num']).size().reset_index(name='count').query('count>0').reset_index(drop=True)\n",
        "display(test)\n",
        "dic_tmin={}\n",
        "for lat, lon in tqdm_notebook(zip(test['lat_num'],test['lon_num'])):\n",
        "    ls=[]\n",
        "    for file in ls_tmin_files:\n",
        "        ls.extend(file.variables['tmin'][:,lat,lon])\n",
        "        #print('done')\n",
        "    dic_tmin[(lat,lon)]=ls\n",
        "\n",
        "import pickle\n",
        "with open(os.path.join(PATH,'dic_tmin_2005_2019'+file_id+'.pkl'), 'wb') as handle:\n",
        "    pickle.dump(dic_tmin, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDzk8qGy_08y",
        "colab_type": "text"
      },
      "source": [
        "# Load necessary dictionary data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWy0eSoXHNTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(PATH,'df_bioclimatic30y'+file_id+'.pkl'), 'rb') as handle:\n",
        "    df_bioclimatic30y=pickle.load(handle)\n",
        "with open(os.path.join(PATH,'dic_prec_2005_2019'+file_id+'.pkl'), 'rb') as handle:\n",
        "    dic_prec = pickle.load(handle)\n",
        "with open(os.path.join(PATH,'dic_tmax_2005_2019'+file_id+'.pkl'), 'rb') as handle:\n",
        "    dic_tmax = pickle.load(handle)\n",
        "with open(os.path.join(PATH,'dic_tmin_2005_2019'+file_id+'.pkl'), 'rb') as handle:\n",
        "    dic_tmin = pickle.load(handle)\n",
        "\n",
        "\n",
        "with open(os.path.join(PATH,'tmin'+'_30y_monthly'+file_id+'.pkl'), 'rb') as handle:\n",
        "    df_tmin30y = pickle.load(handle)\n",
        "with open(os.path.join(PATH,'tmax'+'_30y_monthly'+file_id+'.pkl'), 'rb') as handle:\n",
        "    df_tmax30y = pickle.load(handle)\n",
        "with open(os.path.join(PATH,'tavg'+'_30y_monthly'+file_id+'.pkl'), 'rb') as handle:\n",
        "    df_tavg30y = pickle.load(handle)\n",
        "with open(os.path.join(PATH,'prec'+'_30y_monthly'+file_id+'.pkl'), 'rb') as handle:\n",
        "    df_prec30y = pickle.load(handle)\n",
        "with open(os.path.join(PATH,'srad'+'_30y_monthly'+file_id+'.pkl'), 'rb') as handle:\n",
        "    df_srad30y = pickle.load(handle)\n",
        "with open(os.path.join(PATH,'wind'+'_30y_monthly'+file_id+'.pkl'), 'rb') as handle:\n",
        "    df_wind30y = pickle.load(handle)\n",
        "with open(os.path.join(PATH,'vapr'+'_30y_monthly'+file_id+'.pkl'), 'rb') as handle:\n",
        "    df_vapr30y = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egOSYZn19A08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checklist['lat_lon'] = list(zip(checklist.lat_num, checklist.lon_num))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbIM7GP3_-LC",
        "colab_type": "text"
      },
      "source": [
        "# Convert NaN values to closed ones "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHpL8Y2MQauV",
        "colab_type": "text"
      },
      "source": [
        "## For 30y monthly data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L47ZLdsS_mUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DFs=[df_tmin30y,\n",
        "df_tmax30y,\n",
        "df_tavg30y,\n",
        "df_prec30y,\n",
        "df_srad30y,\n",
        "df_wind30y,\n",
        "df_vapr30y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua00bO1p7jIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check if there are any NaN values\n",
        "for DF in DFs:\n",
        "    print(DF.iloc[:,4].isnull().value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDchrpdd66Gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check if there are any NaN like values\n",
        "for DF in DFs:\n",
        "    print(DF.iloc[:,4].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Mv9hZY666c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert NaN like data to np.nan\n",
        "df_prec30y.loc[:,'prec_30y_monthly']=df_prec30y.prec_30y_monthly.replace(-32768, np.nan)\n",
        "df_srad30y.loc[:,'srad_30y_monthly']=df_srad30y.srad_30y_monthly.replace(65535, np.nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G-uuDNWCq_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def Check_NaN_30ydata(DF):\n",
        "    yy=0\n",
        "    for i in range(len(DF)):\n",
        "        if math.isnan(DF.iloc[i,4]):\n",
        "            yy+=1\n",
        "            print(f\"Row:{i}\")\n",
        "            for j in [0.5,-0.5,1.0,-1.0,1.5,-1.5,2.0,-2.0]:\n",
        "                check_nan=DF[(DF['lat_cate_val']==DF['lat_cate_val'][i])&\n",
        "                             (DF['lon_cate_val']==DF['lon_cate_val'][i]+j)&\n",
        "                             (DF['month']==DF['month'][i])]\n",
        "                try:\n",
        "                    if math.isnan(check_nan.iloc[:,4])==False:\n",
        "                        print(f'replace at{j}')\n",
        "                        DF.iloc[i,4]=check_nan.iloc[:,4].values\n",
        "                        break\n",
        "                except:\n",
        "                    print(f\"Row:{i},Find:{j}, None!\")\n",
        "                    continue\n",
        "    print(f'total:{yy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-UiepQpy-NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check\n",
        "for DF in DFs:\n",
        "    display(DF)\n",
        "    Check_NaN_30ydata(DF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2QImMT04f5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check\n",
        "for DF in DFs:\n",
        "    print(DF.iloc[:,4].isnull().value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEjlTw5j_ri4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for DF in DFs:\n",
        "    checklist=pd.merge(checklist, DF, on=['lat_cate_val','lon_cate_val','month'], how=\"left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYN8yyXZzNw6",
        "colab_type": "text"
      },
      "source": [
        "## For 30y annual data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFLbhoCfCC1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "yy=0\n",
        "for i in range(len(df_bioclimatic30y)):\n",
        "    if math.isnan(df_bioclimatic30y['bio1'][i]):\n",
        "        yy+=1\n",
        "        print(i)\n",
        "        for j in [0.5,-0.5,1.0,-1.0,1.5,-1.5,2.0,-2.0]:\n",
        "            check_nan=df_bioclimatic30y[(df_bioclimatic30y['lat_cate_val']==df_bioclimatic30y['lat_cate_val'][i])&\n",
        "                        (df_bioclimatic30y['lon_cate_val']==df_bioclimatic30y['lon_cate_val'][i]+j)]\n",
        "            try:\n",
        "                if math.isnan(check_nan['bio1'])==False:\n",
        "                    print(j)\n",
        "                    display(check_nan.loc[:,'bio1':])\n",
        "\n",
        "                    for bio_name in list(check_nan.loc[:,'bio1':]):\n",
        "                        #print(bio_name)\n",
        "                        df_bioclimatic30y.loc[(df_bioclimatic30y['lat_cate_val']==df_bioclimatic30y['lat_cate_val'][i])&\n",
        "                            (df_bioclimatic30y['lon_cate_val']==df_bioclimatic30y['lon_cate_val'][i]),bio_name]=check_nan.loc[:,bio_name].values\n",
        "                    break\n",
        "            except:\n",
        "                print(f\"{j} None!\")\n",
        "                continue\n",
        "        #print('end at',j)\n",
        "print(f'total:{yy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unjDBNa2LKa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checklist=pd.merge(checklist, df_bioclimatic30y, on=['lat_cate_val','lon_cate_val'], how=\"outer\") #\"outer\"で大丈夫だと思う。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-r2HqCPFVhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checklist.drop(columns=['count_x','count_y','count'],inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5hgigbJQqhf",
        "colab_type": "text"
      },
      "source": [
        "## For the short term climatic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0tiD7m_u1It",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check masked data----> need to change the coordinate\n",
        "j=0\n",
        "for k in dic_tmin.keys():\n",
        "    if dic_tmin[k][0] is np.ma.masked:\n",
        "        j+=1\n",
        "        print(k)\n",
        "print(j)\n",
        "\n",
        "j=0\n",
        "for k in dic_tmin.keys():\n",
        "    if dic_tmin[k][2000] is np.ma.masked:\n",
        "        j+=1\n",
        "        print(k)\n",
        "print(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egLDmYyD9fHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#====Select one of them\n",
        "#For BCR27\n",
        "map_lat_lon={(104, 567):(104, 566),\n",
        " (105, 567):(105, 566),\n",
        " (106, 568):(106, 567),\n",
        " (107, 568):(107, 567),\n",
        " (108, 569):(108, 568),\n",
        " (109, 567):(109, 566),\n",
        " (109, 568):(109, 566),\n",
        " (109, 569):(109, 566),\n",
        " (110, 567):(110, 566),\n",
        " (111, 565):(111, 564),\n",
        " (112, 563):(112, 562),\n",
        " (112, 564):(112, 562),\n",
        " (114, 560):(114, 559),\n",
        " (115, 559):(115, 558),\n",
        " (116, 558):(116, 557),\n",
        " (117, 557):(117, 556),\n",
        " (118, 557):(118, 556),\n",
        " (119, 542):(119, 541),\n",
        " (119, 543):(119, 544),\n",
        " (119, 545):(119, 544),\n",
        " (119, 546):(119, 548),\n",
        " (119, 547):(119, 548),\n",
        " (119, 557):(119, 556),\n",
        " (120, 551):(120, 550),\n",
        " (120, 552):(120, 553),\n",
        " (121, 553):(121, 554)}\n",
        "\n",
        "#For BCR32\n",
        "map_lat_lon={(117, 486):(117, 487),\n",
        "(116, 486):(116, 487),\n",
        "(115, 485):(115, 486),\n",
        "(114, 482):(114, 485),\n",
        "(114, 483):(114, 485),\n",
        "(113, 480):(113, 485),\n",
        "(113, 481):(113, 485),\n",
        "(113, 482):(113, 485),\n",
        "(113, 483):(113, 485),\n",
        "(113, 484):(113, 485),\n",
        "(112, 479):(112, 483),\n",
        "(112, 480):(112, 483),\n",
        "(111, 479):(111, 481),\n",
        "(111, 480):(111, 481),\n",
        "(110, 478):(110, 479),\n",
        "(109, 477):(109, 478),\n",
        "(106, 475):(106, 476),\n",
        "(104, 473):(104, 475),\n",
        "(104, 474):(104, 475),\n",
        "(103, 473):(103, 474)}\n",
        "\n",
        "checklist['lat_lon_modi']=checklist['lat_lon'].apply(lambda x: map_lat_lon[x] if x in set(map_lat_lon.keys()) else x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdrNAM8OAK-5",
        "colab_type": "text"
      },
      "source": [
        "# Calculate the short term climatic data for our purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQxQCYVQEsGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If there are no missing data in the short term climatic data, this code below is needed!\n",
        "#Otherwise, ignore this.\n",
        "checklist['lat_lon_modi']=checklist['lat_lon'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO5phbfKbYyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_prec(x):\n",
        "    ls=dic_prec[x['lat_lon_modi']][x['day_since_2005']-1825:x['day_since_2005']]\n",
        "\n",
        "    mean30=np.nanmean(ls[-30:])\n",
        "    std30=np.nanstd(ls[-30:])\n",
        "    mean180=np.nanmean(ls[-180:])\n",
        "    std180=np.nanstd(ls[-180:])\n",
        "    mean365=np.nanmean(ls[-365:])\n",
        "    std365=np.nanstd(ls[-365:])\n",
        "    mean730=np.nanmean(ls[-730:])\n",
        "    std730=np.nanstd(ls[-730:])\n",
        "    mean1095=np.nanmean(ls[-1095:])\n",
        "    std1095=np.nanstd(ls[-1095:])\n",
        "    mean1460=np.nanmean(ls[-1460:])\n",
        "    std1460=np.nanstd(ls[-1460:])\n",
        "    mean1825=np.nanmean(ls)\n",
        "    std1825=np.nanstd(ls)\n",
        "\n",
        "    return mean30, std30, mean180, std180, mean365, std365, mean730, std730, mean1095, std1095,mean1460, std1460, mean1825, std1825\n",
        "\n",
        "col_names=['prec30_mean', 'prec30_std','prec180_mean','prec180_std',\n",
        "           'prec365_mean','prec365_std','prec730_mean', 'prec730_std',\n",
        "           'prec1095_mean', 'prec1095_std','prec1460_mean','prec1460_std',\n",
        "           'prec1825_mean','prec1825_std']\n",
        "           \n",
        "df_tocopy=pd.DataFrame(checklist.swifter.apply(cal_prec, axis=1).tolist(), columns=col_names)\n",
        "checklist=pd.concat([checklist, df_tocopy], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY5Mknrfew4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_tmp(x):\n",
        "    ls=[(x + y)/2 for (x, y) in zip(dic_tmax[x['lat_lon_modi']][x['day_since_2005']-1825:x['day_since_2005']],\n",
        "                                    dic_tmin[x['lat_lon_modi']][x['day_since_2005']-1825:x['day_since_2005']])]\n",
        "    mean30=np.nanmean(ls[-30:])\n",
        "    std30=np.nanstd(ls[-30:])\n",
        "    mean180=np.nanmean(ls[-180:])\n",
        "    std180=np.nanstd(ls[-180:])\n",
        "    mean365=np.nanmean(ls[-365:])\n",
        "    std365=np.nanstd(ls[-365:])\n",
        "    mean730=np.nanmean(ls[-730:])\n",
        "    std730=np.nanstd(ls[-730:])\n",
        "    mean1095=np.nanmean(ls[-1095:])\n",
        "    std1095=np.nanstd(ls[-1095:])\n",
        "    mean1460=np.nanmean(ls[-1460:])\n",
        "    std1460=np.nanstd(ls[-1460:])\n",
        "    mean1825=np.nanmean(ls)\n",
        "    std1825=np.nanstd(ls)\n",
        "\n",
        "    return mean30, std30, mean180, std180, mean365, std365, mean730, std730, mean1095, std1095,mean1460, std1460, mean1825, std1825\n",
        "\n",
        "col_names=['tmp30_mean', 'tmp30_std','tmp180_mean','tmp180_std',\n",
        "           'tmp365_mean','tmp365_std','tmp730_mean', 'tmp730_std',\n",
        "           'tmp1095_mean', 'tmp1095_std','tmp1460_mean','tmp1460_std',\n",
        "           'tmp1825_mean','tmp1825_std']\n",
        "\n",
        "df_tocopy=pd.DataFrame(checklist.swifter.apply(cal_tmp, axis=1).tolist(), columns=col_names)\n",
        "checklist=pd.concat([checklist, df_tocopy], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj1xREWyhHm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_tmax(x):\n",
        "    ls=dic_tmax[x['lat_lon_modi']][x['day_since_2005']-1825:x['day_since_2005']]\n",
        "\n",
        "    mean30=np.nanmean(ls[-30:])\n",
        "    std30=np.nanstd(ls[-30:])\n",
        "    mean180=np.nanmean(ls[-180:])\n",
        "    std180=np.nanstd(ls[-180:])\n",
        "    mean365=np.nanmean(ls[-365:])\n",
        "    std365=np.nanstd(ls[-365:])\n",
        "    mean730=np.nanmean(ls[-730:])\n",
        "    std730=np.nanstd(ls[-730:])\n",
        "    mean1095=np.nanmean(ls[-1095:])\n",
        "    std1095=np.nanstd(ls[-1095:])\n",
        "    mean1460=np.nanmean(ls[-1460:])\n",
        "    std1460=np.nanstd(ls[-1460:])\n",
        "    mean1825=np.nanmean(ls)\n",
        "    std1825=np.nanstd(ls)\n",
        "\n",
        "    return mean30, std30, mean180, std180, mean365, std365, mean730, std730, mean1095, std1095,mean1460, std1460, mean1825, std1825\n",
        "\n",
        "col_names=['tmax30_mean', 'tmax30_std','tmax180_mean','tmax180_std',\n",
        "           'tmax365_mean','tmax365_std','tmax730_mean', 'tmax730_std',\n",
        "           'tmax1095_mean', 'tmax1095_std','tmax1460_mean','tmax1460_std',\n",
        "           'tmax1825_mean','tmax1825_std']\n",
        "\n",
        "df_tocopy=pd.DataFrame(checklist.swifter.apply(cal_tmax, axis=1).tolist(), columns=col_names)\n",
        "checklist=pd.concat([checklist, df_tocopy], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eExE41MzhuNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_tmin(x):\n",
        "    ls=dic_tmin[x['lat_lon_modi']][x['day_since_2005']-1825:x['day_since_2005']]\n",
        "\n",
        "    mean30=np.nanmean(ls[-30:])\n",
        "    std30=np.nanstd(ls[-30:])\n",
        "    mean180=np.nanmean(ls[-180:])\n",
        "    std180=np.nanstd(ls[-180:])\n",
        "    mean365=np.nanmean(ls[-365:])\n",
        "    std365=np.nanstd(ls[-365:])\n",
        "    mean730=np.nanmean(ls[-730:])\n",
        "    std730=np.nanstd(ls[-730:])\n",
        "    mean1095=np.nanmean(ls[-1095:])\n",
        "    std1095=np.nanstd(ls[-1095:])\n",
        "    mean1460=np.nanmean(ls[-1460:])\n",
        "    std1460=np.nanstd(ls[-1460:])\n",
        "    mean1825=np.nanmean(ls)\n",
        "    std1825=np.nanstd(ls)\n",
        "\n",
        "    return mean30, std30, mean180, std180, mean365, std365, mean730, std730, mean1095, std1095,mean1460, std1460, mean1825, std1825\n",
        "\n",
        "col_names=['tmin30_mean', 'tmin30_std','tmin180_mean','tmin180_std',\n",
        "           'tmin365_mean','tmin365_std','tmin730_mean', 'tmin730_std',\n",
        "           'tmin1095_mean', 'tmin1095_std','tmin1460_mean','tmin1460_std',\n",
        "           'tmin1825_mean','tmin1825_std']\n",
        "\n",
        "df_tocopy=pd.DataFrame(checklist.swifter.apply(cal_tmin, axis=1).tolist(), columns=col_names)\n",
        "checklist=pd.concat([checklist, df_tocopy], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUD7DA79C0QU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var_names=['prec']\n",
        "var_nums=['30','180','365','730','1095','1460','1825']\n",
        "\n",
        "for var_name in tqdm_notebook(var_names):\n",
        "    for var_num in var_nums:\n",
        "        checklist[var_name+var_num+'_cv']=checklist[var_name+var_num+'_std']/checklist[var_name+var_num+'_mean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLatLY3kA3sy",
        "colab_type": "text"
      },
      "source": [
        "# Merge dataframe and export the climatic variables of our study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4zBedYnA1sp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MergeDF(tmp,checklist):\n",
        "    print(f'Original DF length:{len(tmp)}')\n",
        "    print(f'Climatic DF lenght:{len(checklist)}')\n",
        "    combined=pd.merge(tmp,checklist) \n",
        "  \n",
        "    combined.drop(columns=['year_month', 'day', 'longitude360', 'lat_num','lat_cate',\n",
        "                'lon_num', 'lon_cate','day_since_2009','day_since_2007'],inplace=True)\n",
        "\n",
        "    print(f'Combined DF length:{len(combined)}')\n",
        "    combined.to_csv(os.path.join(PATH,file_id+'_with_prep_tmp_clm.csv'),index=False)\n",
        "    display(combined)\n",
        "    return combined\n",
        "\n",
        "combined=MergeDF(tmp,checklist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrm8zsQDA13f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check\n",
        "pd.set_option('display.max_rows', 130)\n",
        "combined.isnull().any()[combined.isnull().any()==True]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}